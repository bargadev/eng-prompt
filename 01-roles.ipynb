{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27995a75-85b8-48c2-92df-00a911c12faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.environ.get(\"GROQ_API\")  # This is the default and can be omitted\n",
    "\n",
    "print(api_key)\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b828de7-5908-483d-b181-3e1c74cd2cbf",
   "metadata": {},
   "source": [
    "### Compreendendo os Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075789cf-e5f5-424e-8344-bf4f676479ac",
   "metadata": {},
   "source": [
    "- User: Mensagem do usuário\n",
    "- Assistant: Resposta do modelo\n",
    "- System: Instruções de comportamento para o modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5263488-d3d2-43b0-8443-f1d8be489e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LLMs (Large Language Models) – Modelos de Linguagem de Grande Porte**\n",
      "\n",
      "- **O que são?**  \n",
      "  São redes neurais treinadas com enormes volumes de texto (bilhões de palavras) para aprender padrões, gramática, fatos e até estilos de escrita.\n",
      "\n",
      "- **Como funcionam?**  \n",
      "  Recebem uma sequência de palavras (prompt) e, usando probabilidades aprendidas, predizem a próxima palavra, gerando texto coerente e contextualmente adequado.\n",
      "\n",
      "- **Principais características:**  \n",
      "  - **Escala:** milhões a bilhões de parâmetros (os “pesos” da rede).  \n",
      "  - **Versatilidade:** respondem perguntas, resumem, traduzem, escrevem código, criam histórias, etc.  \n",
      "  - **Aprendizado não supervisionado:** grande parte do treinamento ocorre sem rótulos explícitos, apenas a partir do texto bruto.\n",
      "\n",
      "- **Exemplos famosos:** GPT‑4 (OpenAI), LLaMA (Meta), PaLM (Google), Claude (Anthropic).\n",
      "\n",
      "- **Limitações:** podem gerar informações incorretas (alucinações), reproduzir vieses presentes nos dados de treinamento e não possuem compreensão real do mundo, apenas simulação baseada em padrões.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Me explique de forma reduzida o que são as LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e89555-decb-44c6-9ccd-99df13343f75",
   "metadata": {},
   "source": [
    "### Definindo um comportamento para modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7103f4-ea77-4291-bece-667bf6a8f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LLM (Large Language Model)** são modelos de inteligência artificial treinados com enormes quantidades de texto para aprender padrões da linguagem. Eles funcionam como “previsores de palavras”: recebem um trecho de texto e, com base no que aprenderam, geram a continuação mais provável, podendo responder perguntas, resumir, traduzir, criar textos, etc.  \n",
      "\n",
      "- **Escala:** milhões a bilhões de parâmetros (pesos) que capturam relações complexas entre palavras e conceitos.  \n",
      "- **Treinamento:** uso de grandes corpora de texto da internet, livros, artigos, etc., geralmente com aprendizado não supervisionado.  \n",
      "- **Aplicação:** assistentes de conversa, geração de conteúdo, apoio à pesquisa, automação de tarefas de linguagem, entre outros.  \n",
      "\n",
      "Em resumo, uma LLM é um modelo de IA que entende e produz linguagem natural ao prever sequências de texto a partir de vasto conhecimento extraído de dados textuais.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Você é um especialista em IA que responde de forma clara e objetiva.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Me explique de forma reduzida o que são as LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f3cb0-d4d8-44a5-8630-9f6a9b32964c",
   "metadata": {},
   "source": [
    "### Controlando Resposta do Modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57185fbb-6f01-4c9b-81cf-f371c98b95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Aplicações práticas que exigem **baixa latência** em LLMs\n",
      "\n",
      "| Área | Por que a latência importa? | Exemplo concreto |\n",
      "|------|-----------------------------|-------------------|\n",
      "| **Assistentes de voz e IA conversacional** | O usuário fala e espera uma resposta quase instantânea; atrasos > 200 ms já são percebidos como “lento”. | Alexa, Google Assistant ou chatbots de e‑commerce que respondem em < 100 ms para consultas de estoque ou agendamento. |\n",
      "| **Suporte ao cliente em tempo real** | Operadores humanos dependem de sugestões de respostas rápidas para manter o ritmo da conversa. | Sistema de “suggested replies” que gera respostas de 2‑3 frases em < 150 ms durante chats de help‑desk. |\n",
      "| **Tradução simultânea** | Em reuniões ou transmissões ao vivo, a tradução precisa acontecer praticamente em tempo real. | Ferramenta de legendas automáticas que traduz fala de inglês para português com atraso < 300 ms. |\n",
      "| **Jogos e realidade aumentada/virtual (AR/VR)** | Decisões de IA que influenciam a jogabilidade ou a interação do usuário precisam ser instantâneas. | NPCs que geram diálogos dinâmicos em < 50 ms, ou filtros de AR que respondem ao texto falado do usuário sem atraso perceptível. |\n",
      "| **Financeiro e trading algorítmico** | Estratégias automatizadas baseiam‑se em análise de linguagem (notícias, relatórios) e precisam reagir em milissegundos. | Sistema que avalia manchetes de notícias e ajusta ordens de compra/venda em < 100 ms. |\n",
      "| **Sistemas de recomendação em tempo real** | Quando o usuário interage (ex.: adiciona um item ao carrinho), a recomendação deve aparecer imediatamente. | Recomendações de produtos ou vídeos que são recalculadas a cada clique com latência < 80 ms. |\n",
      "| **IoT e edge computing** | Dispositivos com recursos limitados não podem esperar por chamadas à nuvem; precisam de inferência local. | Câmeras de segurança que detectam linguagem ofensiva em áudio e acionam alarme em < 200 ms, rodando o modelo no próprio dispositivo. |\n",
      "| **Educação e tutoria personalizada** | Feedback imediato mantém o fluxo de aprendizagem. | Plataforma que corrige respostas de redação ou resolve dúvidas de matemática em < 150 ms, permitindo que o estudante continue sem interrupções. |\n",
      "| **Saúde e diagnóstico assistido** | Em triagens ou consultas rápidas, respostas rápidas são cruciais para a tomada de decisão. | Assistente que analisa a descrição de sintomas e sugere possíveis causas em < 200 ms, ajudando médicos a priorizar atendimentos. |\n",
      "| **Automação de processos corporativos (RPA)** | Bots que leem documentos e executam ações precisam ser ágeis para não criar gargalos. | Bot que extrai informações de um contrato e preenche campos em um ERP em < 100 ms. |\n",
      "\n",
      "#### Por que a latência baixa faz diferença?\n",
      "1. **Experiência do usuário** – Delays perceptíveis aumentam frustração e abandono.\n",
      "2. **Eficiência operacional** – Processos automáticos que esperam menos tempo podem ser escalados mais barato.\n",
      "3. **Segurança e conformidade** – Em ambientes críticos (financeiro, saúde) a rapidez pode ser parte do requisito regulatório.\n",
      "4. **Viabilidade em edge** – Modelos otimizados para baixa latência rodam em hardware local, reduzindo dependência de conexão de rede.\n",
      "\n",
      "#### Estratégias para alcançar baixa latência\n",
      "- **Modelos compactos / quantizados** (e.g., 4‑bit, 8‑bit).\n",
      "- **Distilação de conhecimento** (student‑teacher).\n",
      "- **Inferência em hardware especializado** (GPU, TPU, ASIC, NPU).\n",
      "- **Cache de resultados** para consultas frequentes.\n",
      "- **Arquiteturas “pipeline”** que processam streaming de texto ao invés de blocos completos.\n",
      "\n",
      "Em resumo, aplicações que exigem interação em tempo real, decisões críticas ou operação em dispositivos com recursos limitados são as que mais se beneficiam de LLMs de baixa latência.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Você é um especialista em IA que responde de forma clara e objetiva.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explique a importância de LLMs com baixa latência.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"LLMs com baixa latência permitem respostas mais rápidas, o que melhora a experiência do usuário.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Pode dar exemplos de aplicações práticas?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab510f3a-6182-42eb-9b91-c4b14380b777",
   "metadata": {},
   "source": [
    "### Usando System Prompt Refinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83318a14-bc6b-481b-96c3-fb1593ed9cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Resumo (até 3 linhas)**  \n",
      "LLMs (Large Language Models) com baixa latência respondem quase que instantaneamente, o que garante melhor experiência do usuário, aumenta a produtividade e viabiliza integrações em tempo real em aplicações críticas de negócios e tecnologia.  \n",
      "\n",
      "---\n",
      "\n",
      "## Por que a latência baixa importa?\n",
      "\n",
      "| Aspecto | Como a latência afeta | Consequência prática |\n",
      "|---|---|---|\n",
      "| **Experiência do usuário** | Cada segundo a mais que a resposta demora pode gerar frustração. | Usuários abandonam chats, assistentes virtuais ou ferramentas de suporte. |\n",
      "| **Produtividade** | Em fluxos de trabalho onde o modelo auxilia a escrita, código ou análise, atrasos interrompem o ritmo. | Equipes gastam mais tempo esperando e menos produzindo. |\n",
      "| **Decisões em tempo real** | Modelos que alimentam sistemas de recomendação, negociação ou monitoramento precisam agir imediatamente. | Perda de oportunidades de venda, risco de decisões atrasadas em trading ou segurança. |\n",
      "| **Escalabilidade de custos** | Alta latência costuma exigir recursos mais potentes ou múltiplas chamadas para compor respostas. | Maior gasto com servidores e energia. |\n",
      "\n",
      "---\n",
      "\n",
      "## Exemplos reais\n",
      "\n",
      "### 1. **Assistentes de Atendimento ao Cliente**\n",
      "- **Situação:** Uma empresa usa um chatbot alimentado por LLM para responder dúvidas de clientes 24/7.  \n",
      "- **Impacto da latência:** Se o bot demora 2‑3 s para responder, o cliente pode ficar impaciente e buscar outro canal (telefone, concorrente).  \n",
      "- **Resultado com baixa latência:** Respostas em < 300 ms mantêm a conversa fluida, aumentam a taxa de resolução no primeiro contato (FCR) e reduzem custos operacionais.\n",
      "\n",
      "### 2. **Ferramentas de Codificação Assistida (e.g., GitHub Copilot)**\n",
      "- **Situação:** Desenvolvedores recebem sugestões de código enquanto digitam.  \n",
      "- **Impacto da latência:** Uma sugestão que chega após 1 s pode interromper o fluxo mental do programador, diminuindo a eficiência.  \n",
      "- **Resultado com baixa latência:** Sugestões quase instantâneas (< 200 ms) mantêm a “corrente” criativa, elevando a produtividade em até 30 % segundo estudos internos da Microsoft.\n",
      "\n",
      "### 3. **Plataformas de E‑commerce com Personalização Dinâmica**\n",
      "- **Situação:** O site exibe recomendações de produtos baseadas no histórico do usuário em tempo real.  \n",
      "- **Impacto da latência:** Um atraso de 500 ms pode fazer o usuário perder o interesse antes mesmo de ver a sugestão.  \n",
      "- **Resultado com baixa latência:** Recomendações exibidas em < 100 ms aumentam a taxa de cliques (CTR) e o valor médio do pedido (AOV).\n",
      "\n",
      "### 4. **Sistemas de Trading Algorítmico**\n",
      "- **Situação:** Estratégias automatizadas analisam notícias, relatórios e sentimentos de mercado para executar ordens.  \n",
      "- **Impacto da latência:** Milissegundos contam; atrasos podem transformar uma oportunidade lucrativa em perda.  \n",
      "- **Resultado com baixa latência:** Modelos que entregam análises em < 50 ms permitem decisões quase simultâneas ao evento, melhorando a rentabilidade.\n",
      "\n",
      "---\n",
      "\n",
      "## Como reduzir a latência de um LLM\n",
      "\n",
      "1. **Modelos otimizados (Distilação, Quantização)**\n",
      "   - *Distilação*: treina um modelo menor que “imita” o grande, mantendo boa qualidade, mas com menos cálculos.\n",
      "   - *Quantização*: usa menos bits para representar os pesos (ex.: 8‑bit ao invés de 32‑bit), acelerando a inferência.\n",
      "\n",
      "2. **Hardware especializado**\n",
      "   - GPUs de última geração, TPUs ou ASICs projetados para inferência de redes neurais entregam resultados mais rápidos.\n",
      "\n",
      "3. **Serviços Edge e Caching**\n",
      "   - Colocar o modelo próximo ao usuário (edge computing) diminui o tempo de ida/volta da rede.\n",
      "   - Cache de respostas frequentes ou de partes do prompt pode evitar recomputação completa.\n",
      "\n",
      "4. **Batching inteligente**\n",
      "   - Agrupar várias solicitações em um único lote quando o volume é alto, aproveitando paralelismo sem aumentar o tempo de resposta individual.\n",
      "\n",
      "5. **Arquiteturas “Hybrid”**\n",
      "   - Combinar um modelo grande (para tarefas complexas) com um modelo pequeno (para consultas simples). O pequeno responde rápido; o grande só entra quando necessário.\n",
      "\n",
      "---\n",
      "\n",
      "## Aplicações de negócios que se beneficiam diretamente\n",
      "\n",
      "| Setor | Caso de uso | Benefício da baixa latência |\n",
      "|---|---|---|\n",
      "| **Finanças** | Análise de sentimentos em notícias, alertas de fraude | Decisões de compra/venda mais rápidas, mitigação de risco |\n",
      "| **Saúde** | Assistentes de triagem virtual | Respostas imediatas melhoram a percepção de cuidado e reduzem sobrecarga de call centers |\n",
      "| **Educação** | Tutores IA que corrigem redações ou resolvem exercícios | Feedback instantâneo mantém o engajamento do aluno |\n",
      "| **Logística** | Otimização de rotas baseada em eventos em tempo real | Redução de custos de combustível e tempo de entrega |\n",
      "| **Marketing** | Geração de textos publicitários dinâmicos ao vivo | Campanhas mais ágeis, aumento de conversão |\n",
      "\n",
      "---\n",
      "\n",
      "## Analogia simples\n",
      "\n",
      "Imagine um garçom em um restaurante: se ele demora minutos para trazer o pedido, o cliente pode ficar impaciente e até sair. Um garçom rápido (baixo tempo de serviço) deixa o cliente satisfeito e aumenta a rotatividade das mesas, gerando mais lucro. O LLM com baixa latência funciona como esse garçom veloz, entregando a “informação” no prato do usuário no momento certo.\n",
      "\n",
      "---\n",
      "\n",
      "## Conclusão\n",
      "\n",
      "LLMs de baixa latência são essenciais para transformar a inteligência artificial de um recurso “depois” (processamento em lote) para um parceiro “em tempo real” nas interações humanas e nos processos críticos de negócio. Investir em otimizações de modelo, hardware adequado e arquitetura de implantação traz ganhos de experiência, produtividade e receita que superam os custos iniciais de implementação.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Você é um especialista em Inteligência Artificial focado em explicar conceitos \"\n",
    "                \"de forma clara, estruturada e prática. Sempre inicie suas respostas com um \"\n",
    "                \"resumo em até 3 linhas, depois detalhe com exemplos reais e aplicações no mundo \"\n",
    "                \"dos negócios e tecnologia. Evite termos excessivamente técnicos sem explicação \"\n",
    "                \"e, quando necessário, use analogias simples. \"\n",
    "                \"Se o usuário pedir código, forneça exemplos em Python bem comentados.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explique a importância de LLMs com baixa latência.\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611134fc-2682-427a-96c6-f7169e5aa539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
